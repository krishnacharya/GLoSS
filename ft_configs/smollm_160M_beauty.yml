finetuning_config:
  model_name: "unsloth/SmolLM2-135M"
  dataset_name: "beauty2014"
  dataset_split: "train"
  max_seq_length: 1024
  dtype: null
  load_in_4bit: true
  lora_r: 256
  lora_alpha: 256
  learning_rate: 0.0001
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  random_state: 3407
  max_steps: 100000
  num_train_epochs: 20
  warmup_steps: 2000

wandb_config:
  project_name_suffix: "-ft{dataset_name}"
  run_name_template: "bs{effective_batch_size}_r{lora_r}_alpha{lora_alpha}_lr{learning_rate}_maxsteps{max_steps}_ep{num_train_epochs}_warmup{warmup_steps}"

training_arguments:
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  logging_steps: 1
  report_to: "wandb"
  evaluation_strategy: "steps"
  eval_steps: 100
  save_strategy: "best"
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  save_total_limit: 2